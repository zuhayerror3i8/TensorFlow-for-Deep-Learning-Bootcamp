{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zuhayerror3i8/TensorFlow-for-Deep-Learning-Bootcamp/blob/main/Section%2003%20-%20Neural%20network%20regression%20with%20TensorFlow/01_neural_network_regression_with_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-StwYXHQcmC"
      },
      "source": [
        "# Introduction to Regression with Neural Networks in  TensorFlow\n",
        "\n",
        "There are many definitions for a regression problem. But in our case, we're going to simplify it: predicting a neumerical variable based on some other combination of variables, even shorter...predicting a number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxGIe3wM1Plu"
      },
      "outputs": [],
      "source": [
        "# Import TensorFlow\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T3uXFUz1ksO"
      },
      "source": [
        "## Creating data to view and fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK5NCOFH1vVx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create features\n",
        "X = np.array([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])\n",
        "\n",
        "# Create labels\n",
        "y = np.array([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])\n",
        "\n",
        "# Visualize it\n",
        "plt.scatter(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUNj8Jaj2oMh"
      },
      "outputs": [],
      "source": [
        "y == X + 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "919x_3xQ2wKi"
      },
      "source": [
        "## Input and output shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fLUlWBHIDmU"
      },
      "outputs": [],
      "source": [
        "# Create a demo tensor for our housing price prediction problem\n",
        "house_info = tf.constant([\"bedroom\", \"bathroom\", \"garage\"])\n",
        "house_price = tf.constant([939700])\n",
        "house_info, house_price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucmqD4mKI0fT"
      },
      "outputs": [],
      "source": [
        "X[0], y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aRfSU-dI5SW"
      },
      "outputs": [],
      "source": [
        "X[1], y[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BnVAwxhIpZc"
      },
      "outputs": [],
      "source": [
        "input_shape = X[0].shape\n",
        "output_shape = y[0].shape\n",
        "input_shape, output_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0R_t2VsJJ5i"
      },
      "outputs": [],
      "source": [
        "X[0].ndim, y[0].ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STKN0jsaJLrF"
      },
      "outputs": [],
      "source": [
        "X[0], y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILebAAcEJpnG"
      },
      "outputs": [],
      "source": [
        "# Turn our NumPy arrays into tensors\n",
        "X = tf.constant(X)\n",
        "y = tf.constant(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7QcAT1BJyiZ"
      },
      "outputs": [],
      "source": [
        "input_shape = X[0].shape\n",
        "output_shape = y[0].shape\n",
        "input_shape, output_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZZI1-5oKBAz"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slmzp99jKENp"
      },
      "source": [
        "## Steps in modelling with TensorFlow\n",
        "\n",
        "1. **Creating a model** - define the input and the output layers, as well as the hidden layers of a deep learning model.\n",
        "2. **Compiling a model** - define the loss function (in other words, the function which tells our model how wrong it is) and the optimizer (tells our model how to improve the patterns of its learning) and evaluation mertrics (what we can use to interpret the performance of our model).\n",
        "3. **Fitting a model** - letting the model try to find patterns between X and y (features and labels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDbYRlP5URee"
      },
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create a model using the Sequential API\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model.compile(loss=tf.keras.losses.mae, # mae is the short for mean absolute error\n",
        "              optimizer=tf.keras.optimizers.SGD(), # SDG is the short for Stochastic Gradient Descent\n",
        "              metrics=[\"mae\"])\n",
        "# 3. Fit the model\n",
        "model.fit(tf.expand_dims(X, axis=-1), y, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iR5byBju1Td"
      },
      "outputs": [],
      "source": [
        "# Check out X and y\n",
        "X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwRH3d-uu6Ic"
      },
      "outputs": [],
      "source": [
        "# Try and make a prediction using our model\n",
        "y_pred = model.predict(np.array([17.0]))\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs-5qMmnwLns"
      },
      "source": [
        "## Improving our model\n",
        "\n",
        "We can improve our model, by altering the steps we took to create a model.\n",
        "\n",
        "1. **Creating a model** - here we might add more layers, increase the number of hidden units (all called neurons) within each of the hidden layers, change the activation function of each layer.\n",
        "2. **Compiling a model** - here we might change the optimization function or perhaps the **learning rate** of the optimization function.\n",
        "3. **Fitting a model** - here we might fit a model for more **epochs** (leave it training for longer) or on more data (give the model more examples to learn from)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqRNYs9ozZHP"
      },
      "outputs": [],
      "source": [
        "# Let's rebuild our model\n",
        "\n",
        "# 1. Create the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model.compile(loss=tf.keras.losses.mae,\n",
        "              optimizer=tf.keras.optimizers.SGD(),\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "# 3. Fit the model (this time we'll train for longer)\n",
        "model.fit(tf.expand_dims(X, axis=-1), y, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW15fkMJ0fRh"
      },
      "outputs": [],
      "source": [
        "# Remind ourselves of the data\n",
        "X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGVE4hy60kLn"
      },
      "outputs": [],
      "source": [
        "# Let's see if our model's prediction has improved...\n",
        "model.predict(np.array([17.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jwcXuQy1h34"
      },
      "outputs": [],
      "source": [
        "# Let's see if we can make another to improve our model\n",
        "\n",
        "# 1. Create the model (this time with an extra hidden layer with 100 hidden units)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model.compile(loss=\"mae\",\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "model.fit(tf.expand_dims(X, axis=-1), y, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0KfAM743SsS"
      },
      "outputs": [],
      "source": [
        "# Let's remind ourselves of the data\n",
        "X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJHrRMBK3Xej"
      },
      "outputs": [],
      "source": [
        "model.predict(np.array([17.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating a model\n",
        "\n",
        "In practice, a typical workflow you'll go through when building a neural networks is:\n",
        "\n",
        "```\n",
        "Build a model -> fit it -> evaluate it -> tweak it -> fit it -> evaluate it...\n",
        "```"
      ],
      "metadata": {
        "id": "C_7H65f_7jBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When it comes to evaluation... there are 3 words you should memorize:\n",
        "\n",
        "> \"visualize, visualize, visualize\"\n",
        "\n",
        "It's a good idea to visualize:\n",
        "* The data - what data are we working with? What does it look like?\n",
        "* The model itself - what does our model look like?\n",
        "* The training of a model - how does a model perform while it learns?\n",
        "* The predictions of the model - how do the predictions of a model line up against the ground truth (the original labels)?"
      ],
      "metadata": {
        "id": "h6LwcDPp7lG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a bigger dataset\n",
        "X = tf.range(-100, 100, 4)\n",
        "X"
      ],
      "metadata": {
        "id": "3lCCDCqG7odb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make labels for the dataset\n",
        "y = X + 10\n",
        "y"
      ],
      "metadata": {
        "id": "WKCVcS-976Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(X, y)"
      ],
      "metadata": {
        "id": "Qv1FNmCV8Cyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The 3 sets...\n",
        "\n",
        "* **Training set** - the model learns from this data, which is typically 70-80% of the total data you have available.\n",
        "* **Validation set** - the model gets tuned on this data, which is typically 10-15% of the data available.\n",
        "* **Test set** - the model gets evaluated on this data to test what it has learned, this set is typically 10-15% of the total data available."
      ],
      "metadata": {
        "id": "K4vCmSwa8enp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the length of how many samples we have\n",
        "len(X)"
      ],
      "metadata": {
        "id": "86OWveXk94Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training set and test set\n",
        "\n",
        "X_train = X[:40] # First 40 are training samples (80% of the data)\n",
        "y_train = y[:40]\n",
        "\n",
        "X_test = X[40:] # Last 10 are the testing samples (20% of the data)\n",
        "y_test = y[40:]\n",
        "\n",
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "lBySiJKQ-FM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the data\n",
        "\n",
        "Now we've got our data in training set and test set... Let's visualize it again!"
      ],
      "metadata": {
        "id": "aOCEZL-i_DMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "# Plot training data in blue\n",
        "plt.scatter(X_train, y_train, c='b', label=\"Training data\")\n",
        "\n",
        "# Plot test data in green\n",
        "plt.scatter(X_test, y_test, c='g', label=\"Testing data\")\n",
        "\n",
        "# Show a legend\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Hy1OKqy1_ZLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's have a look at how to build a neural network for our data\n",
        "\n",
        "# 1. Create a model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model.compile(loss=tf.keras.losses.mae,\n",
        "              optimizer=tf.keras.optimizers.SGD(),\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "# model.fit(X_train, y_train, epochs=100)"
      ],
      "metadata": {
        "id": "0YW-qen_Abuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the model"
      ],
      "metadata": {
        "id": "wNmYYiOcBMjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "6WI5yGJOBS_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0], y[0]"
      ],
      "metadata": {
        "id": "HrcDhE4UCj5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a model which build automatically by defining the input_shape argument in the first layer\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create a model (same as above)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, input_shape=[1], name=\"input_layer\"),\n",
        "    tf.keras.layers.Dense(1, name=\"output_layer\")\n",
        "], name=\"model_1\")\n",
        "\n",
        "# 2. Compile the model (same as above)\n",
        "model.compile(loss=tf.keras.losses.mae,\n",
        "              optimizer=tf.keras.optimizers.SGD(),\n",
        "              metrics=[\"mae\"])"
      ],
      "metadata": {
        "id": "JRukZsstBwjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "qIugzuh_C_c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Total params = total number of parameters in the model.\n",
        "* Trainable params - these are the parameters (patterns) the model can update as it trains.\n",
        "* Non-trainable params - these parameters aren't updated during training (this is typical when you bring in already learn pattens or parameters from other models during **transfer learning**)."
      ],
      "metadata": {
        "id": "deWsX21TDroU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's fit our model to the training data\n",
        "model.fit(X_train, y_train, epochs=100, verbose=0)"
      ],
      "metadata": {
        "id": "Lbe3q7vaF8HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a summary of our model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "3_PvkPdCGsNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "\n",
        "plot_model(model=model, show_shapes=True)"
      ],
      "metadata": {
        "id": "5wTebstCHFvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing our model's predictions\n",
        "\n",
        "To visualize predictions, it's a good idea to plot them against the ground labels.\n",
        "\n",
        "Often you'll see this in the form of `y_test` or `y_true` versus `y_pred` (ground truth versus your model's predictions)."
      ],
      "metadata": {
        "id": "pE_THzV9JZBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make some predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "qsA6YKTtJzVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "sJcNhF_vKgjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a plotting function\n",
        "def plot_predictions(train_data=X_train,\n",
        "                     train_labels=y_train,\n",
        "                     test_data=X_test,\n",
        "                     test_labels=y_test,\n",
        "                     predictions=y_pred):\n",
        "  \"\"\"\n",
        "  Plots training data, test data and compares predictions to ground truth labels.\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  # Plot trianing data in blue\n",
        "  plt.scatter(train_data, train_labels, c=\"b\", label=\"Training data\")\n",
        "  # Plot testing data in green\n",
        "  plt.scatter(test_data, test_labels, c='g', label=\"Testing data\")\n",
        "  # Plot model's predictions in red\n",
        "  plt.scatter(test_data, predictions, c=\"r\", label=\"Predictions\")\n",
        "  # Show the ledend\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "nZzeQzzSMkZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(train_data=X_train,\n",
        "                 train_labels=y_train,\n",
        "                 test_data=X_test,\n",
        "                 test_labels=y_test,\n",
        "                 predictions=y_pred)"
      ],
      "metadata": {
        "id": "0ViYHOnBOZd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating our model's predictions with regression evaluation metrics\n",
        "\n",
        "Depending the problem you're working on, there will be different evaluation metrics to evaluate your model's performance.\n",
        "\n",
        "Sinve we're working on a regression, two of the main metrics:\n",
        "* MAE - mean absolute error, \"on average, how wrong is each of the model's predictions\"\n",
        "* MSE - mean squared error, \"square of the average errors\""
      ],
      "metadata": {
        "id": "hvm97tKAPSLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "M6a-Uhx6RmPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean absolute error\n",
        "mae = tf.metrics.MAE(y_true=y_test,\n",
        "                     y_pred=tf.constant(y_pred))\n",
        "mae"
      ],
      "metadata": {
        "id": "oUYg6iTlSLfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.constant(y_pred)"
      ],
      "metadata": {
        "id": "rhHGr-gER_yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "GOHFg0IISFsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.squeeze(y_pred)"
      ],
      "metadata": {
        "id": "9vuk9q27UA7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean absolute error\n",
        "mae = tf.metrics.MAE(y_true=y_test,\n",
        "                     y_pred=tf.squeeze(y_pred))\n",
        "mae"
      ],
      "metadata": {
        "id": "tA8UM1tuUHBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean squared error\n",
        "mse = tf.metrics.MSE(y_true=y_test,\n",
        "                     y_pred=tf.squeeze(y_pred))\n",
        "mse"
      ],
      "metadata": {
        "id": "B81id5urUa5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make some funcitons to reuse MAE and MSE\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "  return tf.metrics.MAE(y_true=y_test,\n",
        "                        y_pred=tf.squeeze(y_pred))\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "  return tf.metrics.MSE(y_true=y_test,\n",
        "                        y_pred=tf.squeeze(y_pred))"
      ],
      "metadata": {
        "id": "4Y4LOb-RVl22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running experiments to improve our model\n",
        "\n",
        "```\n",
        "Build a model -> fit it -> evaluate it -> tweak it -> fit it -> evaluate it...\n",
        "```\n",
        "\n",
        "1. Get more data - get more examples for your model to train on (more opportunities to learn patterns or relationships between features and labels).\n",
        "2. Make your model larger (using a more complex model) - this might come in the form of more hidden units in each layer.\n",
        "3. Train for longer - give your model more of a chance to find patterns in the data.\n",
        "\n",
        "Let's do 3 modelling experiments:\n",
        "\n",
        "1. `model_1` - same as the original model, 1 layer, trained for 100 epochs.\n",
        "2. `model_2` - 2 layers, trained for 100 epochs.\n",
        "3. `model_3` - 2 layers, trianed for 500 epochs."
      ],
      "metadata": {
        "id": "grurGkwIWWVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build `model_1`**"
      ],
      "metadata": {
        "id": "PLj4najPff58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "model_1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_1.compile(loss=tf.keras.losses.mae,\n",
        "                optimizer=tf.keras.optimizers.SGD(),\n",
        "                metrics=[\"mae\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "model_1.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100)"
      ],
      "metadata": {
        "id": "25bap-nYXjtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make and plot predictions of model_1\n",
        "y_pred_1 = model_1.predict(X_test)\n",
        "plot_predictions(predictions=y_pred_1)"
      ],
      "metadata": {
        "id": "MiMSN1FoZgc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model_1 evaluation metrics\n",
        "mae_1 = mae(y_test, y_pred_1)\n",
        "mse_1 = mse(y_test, y_pred_1)\n",
        "mae_1, mse_1"
      ],
      "metadata": {
        "id": "Vv36_Ud2aYTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build `model_2`**\n",
        "* 2 dense layers, trained for 100 epochs"
      ],
      "metadata": {
        "id": "YUo-iKJ7dWYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "model_2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_2.compile(loss=tf.keras.losses.mae,\n",
        "                optimizer=tf.keras.optimizers.SGD(),\n",
        "                metrics=[\"mae\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "model_2.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100)"
      ],
      "metadata": {
        "id": "dPRvm2-hdh0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make and plot predictions of model_2\n",
        "y_pred_2 = model_2.predict(X_test)\n",
        "plot_predictions(predictions=y_pred_2)"
      ],
      "metadata": {
        "id": "nQ_BgoSL8eef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model_2 evaluation metrics\n",
        "mae_2 = mae(y_test, y_pred_2)\n",
        "mse_2 = mse(y_test, y_pred_2)\n",
        "mae_2, mse_2"
      ],
      "metadata": {
        "id": "Q7MPQCV61Kkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build `model_3`**\n",
        "\n",
        "* 2 layers, trained for 500 epochs"
      ],
      "metadata": {
        "id": "0T8qkrBV1l6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create a model\n",
        "model_3 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_3.compile(loss=tf.keras.losses.mae,\n",
        "                optimizer=tf.keras.optimizers.SGD(),\n",
        "                metrics=[\"mae\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "model_3.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=500)"
      ],
      "metadata": {
        "id": "7mRXcDZl1wBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make and plot some predictions\n",
        "y_pred_3 = model_3.predict(X_test)\n",
        "plot_predictions(predictions=y_pred_3)"
      ],
      "metadata": {
        "id": "fc1hf2cd2lis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model_3 evaluation metrics\n",
        "mae_3 = mae(y_test, y_pred_3)\n",
        "mse_3 = mse(y_test, y_pred_3)\n",
        "mae_3, mse_3"
      ],
      "metadata": {
        "id": "CADPQpto3D6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** You should start with small experiments (small models) and make sure they work and then increase their scale when necessary."
      ],
      "metadata": {
        "id": "FWBMHUTu32zE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing the results of our experiments\n",
        "\n",
        "We've run a few experiments let's compare the results."
      ],
      "metadata": {
        "id": "cYFHhs2h3Slf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's compare our model's results using a pandas DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "model_results = [[\"model_1\", mae_1.numpy(), mse_1.numpy()],\n",
        "                 [\"model_2\", mae_2.numpy(), mse_2.numpy()],\n",
        "                 [\"model_3\", mae_3.numpy(), mse_3.numpy()]]\n",
        "\n",
        "all_results = pd.DataFrame(model_results, columns=[\"model\", \"mae\", \"mse\"])\n",
        "all_results"
      ],
      "metadata": {
        "id": "MJ7Mfiql4YQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like `model_1` performed the best..."
      ],
      "metadata": {
        "id": "j6mFE8at5vJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "id": "Z8QoEJ8Q5XKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:** One of your main goals should be to minimize the time between your experiments. The more experiments you do, the more things you'll figure out which doesn't work and in turn, get closer to figuring out what does work. Remember the machine learning practitioner's motto: \"experiment, experiment, experiment\"."
      ],
      "metadata": {
        "id": "f62YLoRl52CR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tracking your experiments\n",
        "\n",
        "One really good habit in machine learning modelling is to track the results of your experiments.\n",
        "\n",
        "And when doing so, it can be tedious if you're running lots of experiments.\n",
        "\n",
        "Luckily, there are tools to help us!\n",
        "\n",
        "**Resources:** As you build more models, you'll want to look into using:\n",
        "\n",
        "* TensorBoard - a component of the TensorFlow library to help track modelling experiments (we'll see this one later).\n",
        "* Weights & Biases - a tool for tracking all of kinds of machine learning experiments (plugs straight into TensorBoard)."
      ],
      "metadata": {
        "id": "SzvT-sqN7Lw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving our models\n",
        "\n",
        "Saving our models allows us to use them outside of Google Colab (or wherever they were trained) such as in a web application or a mobile app.\n",
        "\n",
        "There are two main formats we can save our model's too:\n",
        "\n",
        "1. The SavedModel format\n",
        "2. The HDF5 format"
      ],
      "metadata": {
        "id": "MT08Dd-l8b5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model using the keras format\n",
        "model_1.save(\"best_model_keras_format.keras\")"
      ],
      "metadata": {
        "id": "ZYBa7JdT9TPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model using the HDF5 format\n",
        "model_1.save(\"best_model_HDF5_format.h5\")"
      ],
      "metadata": {
        "id": "ASwEd2RD_Ht5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading in a saved model"
      ],
      "metadata": {
        "id": "mihl-kk2AUCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the keras format model\n",
        "loaded_keras_format = tf.keras.models.load_model(\"/content/best_model_keras_format.keras\")\n",
        "loaded_keras_format.summary()"
      ],
      "metadata": {
        "id": "djhQ39DsAna-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare model_1 predictions with keras format model predictions\n",
        "model_1_pred = model_1.predict(X_test)\n",
        "loaded_keras_model_pred = loaded_keras_format.predict(X_test)\n",
        "model_1_pred == loaded_keras_model_pred"
      ],
      "metadata": {
        "id": "H3SaRMdkBbx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_pred, loaded_keras_model_pred"
      ],
      "metadata": {
        "id": "VZyefeFaCHb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the MAE of model_1 and loaded_keras_model_pred\n",
        "mae(y_true=y_test, y_pred=model_1_pred) == mae(y_true=y_test, y_pred=loaded_keras_model_pred)"
      ],
      "metadata": {
        "id": "SB_U92mdDDmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in a model using HDF5 format\n",
        "loaded_HDF5_format = tf.keras.models.load_model(\"/content/best_model_HDF5_format.h5\")\n",
        "loaded_HDF5_format.summary()"
      ],
      "metadata": {
        "id": "TKIdIByXDX6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "id": "Mbw7vUZhD1W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to see if loaded HDF5 model predictions match model_1\n",
        "model_1_pred = model_1.predict(X_test)\n",
        "loaded_HDF5_model_pred = loaded_HDF5_format.predict(X_test)\n",
        "model_1_pred == loaded_HDF5_model_pred"
      ],
      "metadata": {
        "id": "KT1LPraTD70j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download a model (or any other file) from Google Colab\n",
        "\n",
        "If you want to downlaod your files from Google Colab:\n",
        "\n",
        "1. You can go to the \"Files\" tab and right click on the file you're after and click \"Downlaod\".\n",
        "2. Use code (see the cell below).\n",
        "3. Save it to Google Drive by connecting Google Drive and copying it there (see 2nd code below)."
      ],
      "metadata": {
        "id": "9x1rlGoYFEhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a file from Google Colab\n",
        "from google.colab import files\n",
        "files.download(\"/content/best_model_HDF5_format.h5\")"
      ],
      "metadata": {
        "id": "mGxjwM2gGJ8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save a file from Google Colab to Google Drive (requires mounting Google Drive)\n",
        "# !cp /content/best_model_HDF5_format.h5 \"PATH\""
      ],
      "metadata": {
        "id": "F2-kvWWkGy2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls \"PATH\""
      ],
      "metadata": {
        "id": "73GWfoBRHw81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A larger example"
      ],
      "metadata": {
        "id": "UuvLwp94IKqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "FiNEkgmeJWFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the insurance dataset\n",
        "insurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/refs/heads/master/insurance.csv\")\n",
        "insurance"
      ],
      "metadata": {
        "id": "--DVupYrKZeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try one-hot encode our DataFrame so it's all numbers\n",
        "insurance_one_hot = pd.get_dummies(insurance, dtype=int)\n",
        "insurance_one_hot.head()"
      ],
      "metadata": {
        "id": "RhiEnLs4K_6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create X & y values (features and labels)\n",
        "X = insurance_one_hot.drop(\"charges\", axis=1)\n",
        "y = insurance_one_hot[\"charges\"]"
      ],
      "metadata": {
        "id": "On6vETuIN358"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View X\n",
        "X.head()"
      ],
      "metadata": {
        "id": "gPsOxpCNOU_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View y\n",
        "y.head()"
      ],
      "metadata": {
        "id": "Zbic8csZObf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the training set and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "len(X), len(X_train), len(X_test)"
      ],
      "metadata": {
        "id": "1HFZZzPWN8lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a neural network (sort of like model_[1/2/3] above)\n",
        "\n",
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create a model\n",
        "insurance_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "insurance_model.compile(loss=tf.keras.losses.mae,\n",
        "                        optimizer=tf.keras.optimizers.SGD(),\n",
        "                        metrics=[\"mae\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "insurance_model.fit(X_train, y_train, epochs=100)"
      ],
      "metadata": {
        "id": "kpQCFiVbN8wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the results of the insurance model on the test data\n",
        "insurance_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "d0HmJdXlRavF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.median(), y_train.mean()"
      ],
      "metadata": {
        "id": "RLxpmB3IRwLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right now it looks like our model isn't performing too well... Let's try to improve it!\n",
        "\n",
        "To imporve our model, we'll run 2 experiments:\n",
        "\n",
        "1. Add an extra layer with more hidden units & use the Adam optimizer.\n",
        "2. Same as above but train for longer.\n",
        "3. Insert your own experiment here."
      ],
      "metadata": {
        "id": "4Yj6ODOMSEM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "insurance_model_2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100),\n",
        "    tf.keras.layers.Dense(10),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "insurance_model_2.compile(loss=tf.keras.losses.mae,\n",
        "                          optimizer=tf.keras.optimizers.Adam(),\n",
        "                          metrics=[\"mae\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "insurance_model_2.fit(X_train, y_train, epochs=100)"
      ],
      "metadata": {
        "id": "i_DIaVtbWrKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "cAWbObGLZEH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_model_2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "YAGPgckzZNek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model (same as above)\n",
        "insurance_model_3 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100),\n",
        "    tf.keras.layers.Dense(10),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "insurance_model_3.compile(loss=tf.keras.losses.mae,\n",
        "                          optimizer=tf.keras.optimizers.Adam(),\n",
        "                          metrics=[\"mae\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "history = insurance_model_3.fit(X_train, y_train, epochs=200)"
      ],
      "metadata": {
        "id": "iiE3MGdYZ3j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "5xzF5_wKa1oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_model_2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "cmsjGI-Qa3Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_model_3.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "y8y2jphva5Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot history (also known as a loss curve or a training curve)\n",
        "pd.DataFrame(history.history).plot()\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epochs\")"
      ],
      "metadata": {
        "id": "djfBmO2SbQtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Question:** How long should you train for?\n",
        "\n",
        "It depends. Really... It depends on the problem you're working on. However, many people have asked this question before... So, TensorFlow has a solution! It's called the `EarlyStopping Callback`, which is a TensorFlow component you can add to your model to stop training once it stops improving a certain metric."
      ],
      "metadata": {
        "id": "N9RwiDHPdBIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing data (normalization and standardization)\n",
        "\n",
        "In terms of scaling  values, neural networks tend to prefer normalization.\n",
        "\n",
        "If you're not sure on which to use, you can try both and see which performs better."
      ],
      "metadata": {
        "id": "OzIMXdpfeLo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Read in the insurance dataframe\n",
        "insurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/refs/heads/master/insurance.csv\")\n",
        "insurance"
      ],
      "metadata": {
        "id": "cVO7-0shhb-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare our data, we can borrow a few classes from Scikit-Learn"
      ],
      "metadata": {
        "id": "IWdJWvNPiRqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a column transformer\n",
        "ct = make_column_transformer(\n",
        "    (MinMaxScaler(), [\"age\", \"bmi\", \"children\"]), # turn all values in these columns between 0 and 1\n",
        "    (OneHotEncoder(handle_unknown=\"ignore\"), [\"sex\", \"smoker\", \"region\"])\n",
        ")\n",
        "\n",
        "# Create X and Y\n",
        "X = insurance.drop(\"charges\", axis=1)\n",
        "y = insurance[\"charges\"]\n",
        "\n",
        "# Build our training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the column transformer to our training data\n",
        "ct.fit(X_train)\n",
        "\n",
        "# Transform training data and test data with normalization (MinMaxScaler) and OneHotEncoder\n",
        "X_train_normal = ct.transform(X_train)\n",
        "X_test_normal = ct.transform(X_test)"
      ],
      "metadata": {
        "id": "JusJ8P5xfwcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does our data look like now?\n",
        "X_train.loc[0]"
      ],
      "metadata": {
        "id": "gmHR8Afnmhay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_normal[0]"
      ],
      "metadata": {
        "id": "vCGwHU6Gmo70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_train_normal.shape"
      ],
      "metadata": {
        "id": "cPYBc8aYnJmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beautiful! Our data has been normalized and one hot encoded. Now let's build a neural network model on it and see how it goes."
      ],
      "metadata": {
        "id": "No3eU6osnW08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a neural network model to fit on our normalized data\n",
        "\n",
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "insurance_model_4 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100),\n",
        "    tf.keras.layers.Dense(10),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "insurance_model_4.compile(loss=tf.keras.losses.mae,\n",
        "                          optimizer=tf.keras.optimizers.Adam(),\n",
        "                          metrics=[\"mae\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "insurance_model_4.fit(X_train_normal, y_train, epochs=100)"
      ],
      "metadata": {
        "id": "x_BQx7sonynb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Insurance model 2 results\n",
        "# 9/9 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5116.0376 - mae: 5116.0376"
      ],
      "metadata": {
        "id": "hQbsJTOAp2JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_model_4.evaluate(X_test_normal, y_test)"
      ],
      "metadata": {
        "id": "XZW0duo0peM_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPH3ofFVV+b4eJWDZ8IPOkO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}